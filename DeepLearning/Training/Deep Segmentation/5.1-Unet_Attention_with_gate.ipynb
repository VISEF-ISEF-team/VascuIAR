{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Model \n",
    "import tensorflow.keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, num_filters): \n",
    "    x = L.Conv2D(num_filters, 3, padding=\"same\")(x) \n",
    "    x = L.BatchNormalization()(x) \n",
    "    x = L.Activation(\"relu\")(x)\n",
    "\n",
    "    x = L.Conv2D(num_filters, 3, padding=\"same\")(x) \n",
    "    x = L.BatchNormalization()(x) \n",
    "    x = L.Activation(\"relu\")(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(x, num_filters): \n",
    "    x = conv_block(x, num_filters)\n",
    "    p = L.MaxPooling2D((2, 2))(x)\n",
    "    return x, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_gate(g, s, num_filters): \n",
    "    Wg = L.Conv2D(num_filters, 1, padding=\"same\")(g) \n",
    "    Wg = L.BatchNormalization()(Wg) \n",
    "\n",
    "    Ws = L.Conv2D(num_filters, 1, padding=\"same\")(s)\n",
    "    Ws = L.BatchNormalization()(Ws)\n",
    "\n",
    "    output = L.Activation(\"relu\")(Wg + Ws) \n",
    "    output = L.Conv2D(num_filters, 1, padding=\"same\")(output) \n",
    "    output = L.Activation(\"sigmoid\")(output) \n",
    "\n",
    "    return output * s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(x, s, num_filters): \n",
    "    x = L.UpSampling2D(interpolation=\"bilinear\")(x) \n",
    "    s = attention_gate(x, s, num_filters)\n",
    "    x = L.Concatenate()([x, s])\n",
    "    x = conv_block(x, num_filters) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention_Unet(input_shape, num_classes): \n",
    "    inputs = L.Input(input_shape) \n",
    "\n",
    "    \"\"\"Encoder\"\"\"\n",
    "    s1, p1 = encoder_block(inputs, 64) \n",
    "    s2, p2 = encoder_block(p1, 128) \n",
    "    s3, p3 = encoder_block(p2, 256) \n",
    "\n",
    "    b1 = conv_block(p3, 512) \n",
    "    \n",
    "    \"\"\"Decoder\"\"\"\n",
    "    d1 = decoder_block(b1, s3, 256) \n",
    "    d2 = decoder_block(d1, s2, 128)\n",
    "    d3 = decoder_block(d2, s1, 64)\n",
    "\n",
    "    \"\"\"Output\"\"\"\n",
    "    outputs = L.Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(d3)\n",
    "\n",
    "    \"\"\"Model\"\"\"\n",
    "    model = Model(inputs, outputs, name=\"Attention-UNet\")\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (600, 600, 512) \n",
    "model = Attention_Unet(input_shape, 11)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
