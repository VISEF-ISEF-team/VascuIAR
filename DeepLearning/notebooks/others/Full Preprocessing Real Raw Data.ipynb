{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pydicom import dcmread\n",
    "import os\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from supporters import *\n",
    "from PIL import Image\n",
    "import SimpleITK as sitk\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Đọc và tiền xử lý ảnh file dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scan(path):\n",
    "    slices = [dcmread(path + '/' + s) for s in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "    try:\n",
    "        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
    "    except:\n",
    "        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
    "    for s in slices:\n",
    "        s.SliceThickness = slice_thickness\n",
    "    return slices\n",
    "\n",
    "\n",
    "def get_pixels_hu(slices):\n",
    "    image = np.stack([s.pixel_array for s in slices])\n",
    "    image = image.astype(np.int16)\n",
    "    image[image == -2000] = 0\n",
    "    for slice_number in range(len(slices)):\n",
    "        intercept = slices[slice_number].RescaleIntercept\n",
    "        slope = slices[slice_number].RescaleSlope\n",
    "        if slope != 1:\n",
    "            image[slice_number] = slope * image[slice_number].astype(np.float64)\n",
    "            image[slice_number] = image[slice_number].astype(np.int16)\n",
    "        image[slice_number] += np.int16(intercept)  \n",
    "    return np.array(image, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load folder tất cả bệnh nhân và load 1 bệnh nhân "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = '../../data/PatientsDCM/'\n",
    "patients = os.listdir(INPUT_FOLDER)\n",
    "patients.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT001_scan = load_scan(INPUT_FOLDER + patients[0])\n",
    "PAT001 = get_pixels_hu(PAT001_scan)\n",
    "PAT001.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_3D_array(PAT001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Thực Hiện Linear Regression để loại bỏ những tấm ảnh gây nhiễu (không chứa tim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('../../data/PatientsDCM/Postprocessing/binary_training_label.csv')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tất cả bộ ảnh của tất cả bệnh nhân và X_train\n",
    "X = []\n",
    "\n",
    "for i in range(0, 16, 1):\n",
    "    PAT = load_scan(INPUT_FOLDER + patients[i])\n",
    "    PAT = get_pixels_hu(PAT)\n",
    "    for j in range(PAT.shape[0]):\n",
    "        X.append(PAT[j])\n",
    "    \n",
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "X_min = np.min(X)\n",
    "X_max = np.max(X)\n",
    "\n",
    "X = (X - X_min) / (X_max - X_min)\n",
    "np.min(X), np.max(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Huấn luyện mô hình với Logistic Regression của scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1 Trình chiếu trên mặt phẳng 2D để xem độ phân biệt của data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# plot the transformed data with different colors for different labels\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Reshape the data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Create a logistic regression object\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "def log_loss(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    loss = -1/n * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "# Calculate loss\n",
    "loss = log_loss(y_test, y_pred)\n",
    "\n",
    "accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def sigmoid(x):\n",
    "  return 1/(1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-6,6,100)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression(X, y, alpha=0.01, num_iterations=100):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    accuracy = []\n",
    "    log_loss = []\n",
    "    for i in range(num_iterations):\n",
    "        z = np.dot(X, theta)\n",
    "        h = sigmoid(z)\n",
    "        J = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        gradient = (1/m) * np.dot(X.T, (h - y))\n",
    "        theta -= alpha * gradient\n",
    "        y_pred = (h >= 0.4).astype(int)\n",
    "        accuracy.append(np.mean(y_pred == y))\n",
    "        log_loss.append(J)\n",
    "    return theta, accuracy, log_loss\n",
    "\n",
    "# Reshape the data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Add bias term to X\n",
    "X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "# Train the model\n",
    "theta, accuracy, log_loss = logistic_regression(X_train, y_train)\n",
    "\n",
    "# Plot the accuracy and loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(accuracy)\n",
    "plt.title('Đường biểu diễn độ chính xác')\n",
    "plt.xlabel('Số lần chạy')\n",
    "plt.ylabel('Chỉ số chính xác')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(log_loss)\n",
    "plt.title('Đường biểu diễn hàm mất mát log')\n",
    "plt.xlabel('Số lần chạy')\n",
    "plt.ylabel('Chỉ số mất mát')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "512 * 512 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression(X, y, alpha=0.01, num_iterations=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    accuracy = []\n",
    "    log_loss = []\n",
    "    for i in range(num_iterations):\n",
    "        z = np.dot(X, theta)\n",
    "        h = sigmoid(z)\n",
    "        J = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        gradient = (1/m) * np.dot(X.T, (h - y))\n",
    "        theta -= alpha * gradient\n",
    "        y_pred = (h >= 0.5).astype(int)\n",
    "        accuracy.append(np.mean(y_pred == y))\n",
    "        log_loss.append(J)\n",
    "    return theta, accuracy, log_loss\n",
    "\n",
    "# Reshape the data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Add bias term to X\n",
    "X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "# Train the model\n",
    "theta, accuracy, log_loss = logistic_regression(X_train, y_train)\n",
    "\n",
    "# Plot the accuracy and loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(accuracy)\n",
    "plt.title('Đường biểu diễn độ chính xác')\n",
    "plt.xlabel('Số lần chạy')\n",
    "plt.ylabel('Chỉ số chính xác')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(log_loss)\n",
    "plt.title('Đường biểu diễn hàm mất mát log')\n",
    "plt.xlabel('Số lần chạy')\n",
    "plt.ylabel('Chỉ số mất mát')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PAT001\n",
    "image = image.reshape(image.shape[0],-1)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to disk and reload\n",
    "filename = '../../model/training/models/logreg_model.sav'\n",
    "pickle.dump(logreg, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(image)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = end = 0\n",
    "\n",
    "for index, value in enumerate(y_pred):\n",
    "    if value == 1:\n",
    "        start = index\n",
    "        while y_pred[index] == 1:\n",
    "            index += 1\n",
    "        end = index\n",
    "        break\n",
    "        \n",
    "start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tạo pipeline hoàn chỉnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(image):\n",
    "    # Load model\n",
    "    filename = '../../model/training/models/logreg_model.sav'\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    # Predict\n",
    "    temp = image.reshape(image.shape[0],-1)\n",
    "    y_pred = loaded_model.predict(temp)\n",
    "    \n",
    "    # filtering\n",
    "    start = end = 0\n",
    "    for index, value in enumerate(y_pred):\n",
    "        if value == 1:\n",
    "            start = index\n",
    "            while y_pred[index] == 1:\n",
    "                index += 1\n",
    "            end = index\n",
    "            break\n",
    "            \n",
    "    image = image[start:end]\n",
    "    return image\n",
    "\n",
    "PAT001 = filtering(PAT001)\n",
    "PAT001.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giảm thiểu các tấm ảnh nhiễu rất nhiều\n",
    "explore_3D_array(PAT001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Huấn luyện mô hình với Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "X_max = 1024\n",
    "X_min = -1024\n",
    "\n",
    "X = (X - X_min) / (X_max - X_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "X_val = X_val.reshape(X_val.shape[0], -1) # Reshape the validation data\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and save the history\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val), validation_split=0.1) # Add the validation data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Tập huấn luyện')\n",
    "plt.plot(history.history['accuracy'], label='Độ chính xác')\n",
    "plt.plot(history.history['loss'], label='Độ mất mát')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Tập đánh giá')\n",
    "plt.plot(history.history['val_accuracy'], label='Độ chính xác')\n",
    "plt.plot(history.history['val_loss'], label='Độ mất mát')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../model/training/models/binary_tensor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_tensor = tf.keras.models.load_model('../../model/training/models/binary_tensor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT001 = PAT001.reshape(PAT001.shape[0], -1)\n",
    "y_pred = load_model_tensor.predict(PAT001)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cắt tròn với phương trình đường tròn và lưu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ban_kinh = 190\n",
    "a = b = 512/2\n",
    "\n",
    "left = top = a - ban_kinh\n",
    "right = bottom = a + ban_kinh\n",
    "\n",
    "def euclidian_distance(x, y, a, b):\n",
    "    dis = ((x - a)**2 + (y - b)**2)**(1/2)\n",
    "    return dis\n",
    "\n",
    "crop_imgs = []\n",
    "def circling():\n",
    "    for index in range(PAT001.shape[0]):\n",
    "        img = PAT001[index]\n",
    "        for x in range(PAT001.shape[1]):\n",
    "            for y in range(PAT001.shape[2]):\n",
    "                if euclidian_distance(x, y, a, b) > ban_kinh:\n",
    "                    img[x,y] = -2000\n",
    "        img = img[int(top) : int(bottom), int(left) : int(right)]\n",
    "        crop_imgs.append(img)\n",
    "        \n",
    "circling()\n",
    "crop_imgs = np.array(crop_imgs)\n",
    "crop_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT001 = crop_imgs\n",
    "explore_3D_array(PAT001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(PAT001[100], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lưu lại dưới định dạng .nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '../../data/PatientsDCM/PAT001/PAT001.nii.gz'\n",
    "converted_array = np.array(PAT001, dtype=np.float32)\n",
    "converted_array = np.transpose(converted_array, (2, 1, 0))\n",
    "\n",
    "affine = np.eye(4)\n",
    "nifti_file = nib.Nifti1Image(converted_array, affine)\n",
    "nib.save(nifti_file, out_path)\n",
    "\n",
    "# reread to check\n",
    "raw_img_sitk = sitk.ReadImage(out_path, sitk.sitkFloat32)\n",
    "raw_img_sitk = sitk.GetArrayFromImage(raw_img_sitk)\n",
    "print(f'Shape of numpy array: {raw_img_sitk.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_3D_array_comparison(raw_img_sitk, PAT001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Xem và Normalize đơn vị Housefield\n",
    "\n",
    "Đơn vị Hounsfield (HU) là một đơn vị được sử dụng để thể hiện mật độ phóng xạ của vật liệu trong chụp cắt lớp vi tính (CT). Thang đo HU dựa trên mật độ phóng xạ của không khí và nước, được gán các giá trị lần lượt là -1000 HU và 0 HU. Thang đo HU dao động từ -1000 HU đối với không khí đến +3000 HU đối với xương hoặc kim loại rất dày đặc.  Các vật liệu và mô khác nhau có giá trị HU khác nhau, có thể giúp xác định và phân biệt chúng trên hình ảnh CT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(PAT001), np.min(PAT001))\n",
    "\n",
    "plt.hist(PAT001.flatten(), bins=80, color='c')\n",
    "plt.xlabel(\"Đơn vị Hounsfield\")\n",
    "plt.ylabel(\"Tấn suất\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuy nhiên range Hounsfield Unit ở trên chưa chính xác trong vùng hình tròn và có một vài nhiễu +3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization():\n",
    "    max_val = min_val = 0\n",
    "    # Find max min\n",
    "    for index in range(PAT001.shape[0]):\n",
    "            img = PAT001[index]\n",
    "            for x in range(PAT001.shape[1]):\n",
    "                for y in range(PAT001.shape[2]):\n",
    "                    if euclidian_distance(x, y, a=PAT001.shape[1]/2, b=PAT001.shape[1]/2) <= ban_kinh: # nằm trong hình tròn\n",
    "                        if img[x,y] > max_val:\n",
    "                            max_val = img[x,y] \n",
    "                        elif img[x,y] < max_val:\n",
    "                            min_val = img[x,y] \n",
    "    # Intensity normalization                        \n",
    "    for index in range(PAT001.shape[0]):\n",
    "        img = PAT001[index]\n",
    "        for x in range(PAT001.shape[1]):\n",
    "            for y in range(PAT001.shape[2]):\n",
    "                if euclidian_distance(x, y, a=PAT001.shape[1]/2, b=PAT001.shape[1]/2) <= ban_kinh: # nằm trong hình tròn\n",
    "                    img = (img - min_val) / (max_val - min_val)\n",
    "                    PAT001[index] = img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoặc Normalize với range tùy chọn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "in_range = (-1024, 1024)\n",
    "mask = (PAT001 >= in_range [0]) & (PAT001 <= in_range [1])\n",
    "masked_array = np.ma.masked_array (PAT001, ~mask)\n",
    "normalized_array = stats.zscore (masked_array, axis = 0)\n",
    "normalized_array = normalized_array.filled (0)\n",
    "\n",
    "plt.hist(normalized_array.flatten(), bins=80, color='c')\n",
    "plt.xlabel(\"Đơn vị Hounsfield\")\n",
    "plt.ylabel(\"Tấn suất\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize with keras lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = sitk.ReadImage('../../data/PatientsDCM/Postprocessing/PAT001.nii.gz', sitk.sitkFloat32)\n",
    "img = sitk.GetArrayFromImage(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(img.flatten(), bins=80, color='c')\n",
    "plt.xlabel(\"Đơn vị Hounsfield\")\n",
    "plt.ylabel(\"Tấn suất\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import normalize\n",
    "\n",
    "img = normalize(img, axis=1)\n",
    "print(np.max(img), np.min(img))\n",
    "explore_3D_array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý: Bước normalization chỉ thực hiện khi muốn train một model nào đó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Toàn bộ code chạy một lần các các bộ PAT khác nhau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Các hàm hỗ trợ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = '../../data/PatientsDCM/'\n",
    "patients = os.listdir(INPUT_FOLDER)\n",
    "patients.sort()\n",
    "\n",
    "def load_scan(path):\n",
    "    slices = [dcmread(path + '/' + s) for s in os.listdir(path)]\n",
    "    slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "    try:\n",
    "        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
    "    except:\n",
    "        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
    "    for s in slices:\n",
    "        s.SliceThickness = slice_thickness\n",
    "    return slices\n",
    "\n",
    "\n",
    "def get_pixels_hu(slices):\n",
    "    image = np.stack([s.pixel_array for s in slices])\n",
    "    image = image.astype(np.int16)\n",
    "    image[image == -2000] = 0\n",
    "    for slice_number in range(len(slices)):\n",
    "        intercept = slices[slice_number].RescaleIntercept\n",
    "        slope = slices[slice_number].RescaleSlope\n",
    "        if slope != 1:\n",
    "            image[slice_number] = slope * image[slice_number].astype(np.float64)\n",
    "            image[slice_number] = image[slice_number].astype(np.int16)\n",
    "        image[slice_number] += np.int16(intercept)  \n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "def euclidian_distance(x, y, a, b):\n",
    "    dis = ((x - a)**2 + (y - b)**2)**(1/2)\n",
    "    return dis\n",
    "\n",
    "def filtering(image):\n",
    "    # Load model\n",
    "    filename = '../../model/training/models/logreg_model.sav'\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        \n",
    "    # Predict\n",
    "    temp = image.reshape(image.shape[0],-1)\n",
    "    y_pred = loaded_model.predict(temp)\n",
    "        \n",
    "    # filtering\n",
    "    start = end = 0\n",
    "    for index, value in enumerate(y_pred):\n",
    "        if value == 1:\n",
    "            start = index\n",
    "            while y_pred[index] == 1 and index < image.shape[0] - 1:\n",
    "                index += 1\n",
    "            end = index\n",
    "            break\n",
    "                \n",
    "    image = image[start:end]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code chạy chính"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(index_PAT):\n",
    "    # Read the volumetric images\n",
    "    PAT_scan = load_scan(INPUT_FOLDER + patients[index_PAT - 1])\n",
    "    PAT = get_pixels_hu(PAT_scan)\n",
    "    \n",
    "    # Filtering with logistic regression\n",
    "    PAT = filtering(PAT)\n",
    "\n",
    "    # Circling and clipping\n",
    "    ban_kinh = 190\n",
    "    a = b = 512/2\n",
    "    left = top = a - ban_kinh\n",
    "    right = bottom = a + ban_kinh\n",
    "    crop_imgs = []\n",
    "\n",
    "    for index in range(PAT.shape[0]):\n",
    "        img = PAT[index]\n",
    "        for x in range(PAT.shape[1]):\n",
    "            for y in range(PAT.shape[2]):\n",
    "                if euclidian_distance(x, y, a, b) > ban_kinh:\n",
    "                    img[x,y] = -2000\n",
    "        img = img[int(top) : int(bottom), int(left) : int(right)]\n",
    "        crop_imgs.append(img)\n",
    "    crop_imgs = np.array(crop_imgs)\n",
    "    \n",
    "    # Save into .nii.gz file\n",
    "    index_PAT = '00' + str(index_PAT) if index_PAT < 10 else '0' + str(index_PAT)\n",
    "    out_path = f'../../data/PatientsDCM/Postprocessing/PAT{index_PAT}.nii.gz'\n",
    "    converted_array = np.array(crop_imgs, dtype=np.float32)\n",
    "    converted_array = np.transpose(converted_array, (2, 1, 0))\n",
    "    affine = np.eye(4)\n",
    "    nifti_file = nib.Nifti1Image(converted_array, affine)\n",
    "    nib.save(nifti_file, out_path)\n",
    "    \n",
    "    # Done line\n",
    "    print(f'{index_PAT} done!')\n",
    "\n",
    "# Loop through all patients\n",
    "for index_PAT in range(1, 17, 1):\n",
    "    run(index_PAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Kiểm tra kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = sitk.ReadImage('../../data/PatientsDCM/Postprocessing/PAT004.nii.gz', sitk.sitkFloat32)\n",
    "img = sitk.GetArrayFromImage(img)\n",
    "print(img.shape)\n",
    "explore_3D_array(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
