{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.utils' has no attribute 'generic_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Github\\VascuIAR\\DeepLearning\\model\\4-Multiclass_Unet_using_VGG_Resnet_Inception.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Github/VascuIAR/DeepLearning/model/4-Multiclass_Unet_using_VGG_Resnet_Inception.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Github/VascuIAR/DeepLearning/model/4-Multiclass_Unet_using_VGG_Resnet_Inception.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msegmentation_models\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msm\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Github/VascuIAR/DeepLearning/model/4-Multiclass_Unet_using_VGG_Resnet_Inception.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mglob\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Github/VascuIAR/DeepLearning/model/4-Multiclass_Unet_using_VGG_Resnet_Inception.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n",
      "File \u001b[1;32me:\\Github\\VascuIAR\\.venv\\lib\\site-packages\\segmentation_models\\__init__.py:98\u001b[0m\n\u001b[0;32m     96\u001b[0m _framework \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mSM_FRAMEWORK\u001b[39m\u001b[39m'\u001b[39m, _DEFAULT_KERAS_FRAMEWORK)\n\u001b[0;32m     97\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     set_framework(_framework)\n\u001b[0;32m     99\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     other \u001b[39m=\u001b[39m _TF_KERAS_FRAMEWORK_NAME \u001b[39mif\u001b[39;00m _framework \u001b[39m==\u001b[39m _KERAS_FRAMEWORK_NAME \u001b[39melse\u001b[39;00m _KERAS_FRAMEWORK_NAME\n",
      "File \u001b[1;32me:\\Github\\VascuIAR\\.venv\\lib\\site-packages\\segmentation_models\\__init__.py:68\u001b[0m, in \u001b[0;36mset_framework\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m _KERAS_FRAMEWORK_NAME:\n\u001b[0;32m     67\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mefficientnet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m  \u001b[39m# init custom objects\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39m==\u001b[39m _TF_KERAS_FRAMEWORK_NAME:\n\u001b[0;32m     70\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n",
      "File \u001b[1;32me:\\Github\\VascuIAR\\.venv\\lib\\site-packages\\efficientnet\\keras.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m EfficientNetB7 \u001b[39m=\u001b[39m inject_keras_modules(model\u001b[39m.\u001b[39mEfficientNetB7)\n\u001b[0;32m     15\u001b[0m preprocess_input \u001b[39m=\u001b[39m inject_keras_modules(model\u001b[39m.\u001b[39mpreprocess_input)\n\u001b[1;32m---> 17\u001b[0m init_keras_custom_objects()\n",
      "File \u001b[1;32me:\\Github\\VascuIAR\\.venv\\lib\\site-packages\\efficientnet\\__init__.py:71\u001b[0m, in \u001b[0;36minit_keras_custom_objects\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m model\n\u001b[0;32m     66\u001b[0m custom_objects \u001b[39m=\u001b[39m {\n\u001b[0;32m     67\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mswish\u001b[39m\u001b[39m'\u001b[39m: inject_keras_modules(model\u001b[39m.\u001b[39mget_swish)(),\n\u001b[0;32m     68\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFixedDropout\u001b[39m\u001b[39m'\u001b[39m: inject_keras_modules(model\u001b[39m.\u001b[39mget_dropout)()\n\u001b[0;32m     69\u001b[0m }\n\u001b[1;32m---> 71\u001b[0m keras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mgeneric_utils\u001b[39m.\u001b[39mget_custom_objects()\u001b[39m.\u001b[39mupdate(custom_objects)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.utils' has no attribute 'generic_utils'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import keras \n",
    "import SimpleITK as sitk\n",
    "from keras.utils import normalize\n",
    "from keras.metrics import MeanIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the parameters\n",
    "SIZE_X = 128 \n",
    "SIZE_Y = 128\n",
    "n_classes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train and mask files into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((900, 512, 512), (900, 512, 512))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_volumetric_data():\n",
    "    train_path = '../data/MM_WHS/ct_train'\n",
    "    mask_path = '../data/MM_WHS/ct_train_mask'\n",
    "    \n",
    "    train_files = glob.glob(os.path.join(train_path, '*.nii.gz'))\n",
    "    mask_files = glob.glob(os.path.join(mask_path, '*.nii.gz'))\n",
    "\n",
    "    train_images = []\n",
    "    for index_file in range(3):\n",
    "        pat = sitk.ReadImage(train_files[index_file], sitk.sitkFloat32)\n",
    "        pat = sitk.GetArrayFromImage(pat)\n",
    "        for i in range(pat.shape[0]):\n",
    "            train_images.append(pat[i])\n",
    "            \n",
    "    train_masks = []\n",
    "    for index_file in range(3):\n",
    "        pat = sitk.ReadImage(mask_files[index_file], sitk.sitkFloat32)\n",
    "        pat = sitk.GetArrayFromImage(pat)\n",
    "        for i in range(pat.shape[0]):\n",
    "            train_masks.append(pat[i])\n",
    "        \n",
    "    train_images = np.array(train_images)\n",
    "    train_masks = np.array(train_masks)\n",
    "    return train_images, train_masks\n",
    "\n",
    "\n",
    "train_images, train_masks = read_volumetric_data()\n",
    "train_images.shape, train_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = train_masks.shape\n",
    "train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "np.unique(train_masks_encoded_original_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\n",
    "X_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.5, random_state = 0)\n",
    "\n",
    "print(\"Class values in the dataset are ... \", np.unique(y_train))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
    "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
    "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
