{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_multi_unet_model import multi_unet_model # Uses softmax as last activation\n",
    "\n",
    "from keras.utils import normalize\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing images, if needed\n",
    "SIZE_X = 128 \n",
    "SIZE_Y = 128\n",
    "n_classes = 4 # Number of classes for segmentation\n",
    "\n",
    "# Capture training image info as a list\n",
    "train_images = []\n",
    "\n",
    "for directory_path in glob.glob(\"128_patches/images/\"):\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
    "        img = cv2.imread(img_path, 0)       \n",
    "        img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "        train_images.append(img)\n",
    "       \n",
    "# Convert list to array for machine learning processing        \n",
    "train_images = np.array(train_images)\n",
    "\n",
    "# Capture mask/label info as a list\n",
    "train_masks = [] \n",
    "for directory_path in glob.glob(\"128_patches/masks/\"):\n",
    "    for mask_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
    "        mask = cv2.imread(mask_path, 0)       \n",
    "        mask = cv2.resize(mask, (SIZE_X, SIZE_Y), interpolation = cv2.INTER_NEAREST)  # Otherwise ground truth changes due to interpolation\n",
    "        train_masks.append(mask)\n",
    "        \n",
    "# Convert list to array for machine learning processing          \n",
    "train_masks = np.array(train_masks)\n",
    "\n",
    "# Encode labels... but multi dim array so need to flatten, encode and reshape\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = train_masks.shape\n",
    "train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "np.unique(train_masks_encoded_original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "train_images = normalize(train_images, axis=1)\n",
    "\n",
    "train_masks_inputs = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
    "\n",
    "# Create a subset of data for quick testing\n",
    "# Picking 10% for testing and remaining for training (train set + validation set)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.1, random_state = 0)\n",
    "\n",
    "# Further split training data to a smaller subset for quick testing of models\n",
    "X_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled \n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
    "\n",
    "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
    "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
